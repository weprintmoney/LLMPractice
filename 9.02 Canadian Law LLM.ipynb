{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weprintmoney/LLMPractice/blob/main/9.02%20Canadian%20Law%20LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center>Custom RAG Implementation</center></h1>\n",
        "<h2><center>The Consolidated Acts and Regulations of Canada</center></h2>\n",
        "<h3><center>Charlcye Mitchell & Matt Moore, May 2024</center></h3>"
      ],
      "metadata": {
        "id": "KaADFclbgPN9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7Lc9I6taO3k"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/semantic-search.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/docs/semantic-search.ipynb)\n",
        "\n",
        "### Objective\n",
        "The goal of this assignment is to explore advanced applications of large language models in the legal domain. We will implement the LLaMA3 GGUF model with the Retrieval-Augmented Generation (RAG) technique, using a dataset consisting of the consolidated acts and regulations of Canada. This implementation aims to leverage the rich contextual understanding of the LLaMA3 model with the retrieval capabilities of RAG to enhance the accuracy and relevance of generated responses in legal contexts.\n",
        "\n",
        "### Background\n",
        "\n",
        "*   The model we will be downloading from Hugging Face is a **5-bit quantized version of the Llama 3 8B chat model**, made available by NousResearch. The model is made available in the **GGUF format** - a new format introduced by the Llama CPP team and a replacement for the earlier GGML format, with advantages such as better tokenization and support for special tokens. Llama is short for **L**arge **LA**nguage Model **M**eta **A**I. https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct-GGUF\n",
        "\n",
        "*   RAG combines a powerful transformer-based language model with a retrieval system, allowing the model to pull in relevant external information during the generation process. This combination is particularly potent for domains like law where precedent and specific details are crucial. https://www.llamaindex.ai/blog/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b\n",
        "\n",
        "*   FAISS (Facebook AI Similarity Search) empowers us with its state-of-the-art similarity search capabilities, allowing us to effortlessly find documents that closely match a given query. https://python.langchain.com/v0.1/docs/integrations/vectorstores/faiss/\n",
        "\n",
        "*   LangChain equips us with advanced text generation techniques, enabling our query engine to generate meaningful and context-aware responses. https://python.langchain.com/v0.1/docs/get_started/introduction\n",
        "\n",
        "### Dataset\n",
        "\n",
        "*   The legal dataset provided includes the consolidated acts and regulations of Canada in both English and French as a collection of XML documents which are regularly updated in the linked repository. This dataset will serve as the source for the retrieval component of the RAG, enabling the LLaMA3 model to access and utilize specific legal information when generating responses. https://github.com/justicecanada/laws-lois-xml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "If you are running this notebook in Google Colab, begin by verifying you  have set your runtime correctly: Runtime > Change runtime type > T4 GPU (or higher)\n",
        "\n",
        "# Data Download & Preprocessing\n",
        "\n",
        "We will be utilizing FAISS as our vector store. To begin we must install the required prerequisite libraries and process and embed our XML document data into a FAISS index."
      ],
      "metadata": {
        "id": "XZOuBIUZiQFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I borrowed this code from another project in an attempt to understand how to parse all of our XML files into the vector store. We will need to swap the JSON loader for an XML loader at the very least.\n",
        "\n",
        "import functools\n",
        "import pathlib\n",
        "from typing import Any\n",
        "\n",
        "import langchain.text_splitter\n",
        "import langchain_community.document_loaders\n",
        "import langchain_community.embeddings\n",
        "import langchain_community.vectorstores.faiss\n",
        "import langchain_core.documents\n",
        "import sentence_transformers\n",
        "import torch\n",
        "\n",
        "# A custom configuration of the SentenceTransformer class is defined using functools.partial, specifying the transformer should run on the CPU and trust remote code.\n",
        "SentenceTransformer = functools.partial(sentence_transformers.SentenceTransformer, trust_remote_code=True, device=\"cpu\")\n",
        "\n",
        "# This function is intended to update metadata for a record by extracting and modifying metadata information based on the record's contents:\n",
        "# - It adjusts the topic by extracting the parent directory name from the source if available.\n",
        "# - Updates the source and title from the record data.\n",
        "\n",
        "def metadata_func(record: dict[str, Any], metadata: dict[str, Any]) -> dict[str, Any]:\n",
        "    \"\"\"Extract metadata from a record.\n",
        "\n",
        "    Args:\n",
        "        record: The record.\n",
        "        metadata: The default metadata generated by the JSONLoader.\n",
        "\n",
        "    Returns:\n",
        "        The updated metadata.\n",
        "    \"\"\"\n",
        "    if \"source\" in metadata:\n",
        "        metadata[\"topic\"] = pathlib.Path(metadata[\"source\"]).parent.name\n",
        "    metadata[\"source\"] = record.get(\"url\")\n",
        "    metadata[\"title\"] = record.get(\"title\")\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# This function deduplicates document chunks:\n",
        "# - It takes a list of document chunks and filters out duplicates based on the content of the chunks (page_content), ensuring that only unique chunks are retained.\n",
        "\n",
        "def dedup_chunks(\n",
        "    chunks: list[langchain_core.documents.Document],\n",
        ") -> list[langchain_core.documents.Document]:\n",
        "    \"\"\"Deduplicate chunks based on their page content.\n",
        "\n",
        "    Args:\n",
        "        chunks: A list of chunks.\n",
        "\n",
        "    Returns:\n",
        "        A list of deduplicated chunks.\n",
        "    \"\"\"\n",
        "    deduped_chunks = []\n",
        "\n",
        "    chunk_set = set()\n",
        "    for chunk in chunks:\n",
        "        if chunk.page_content not in chunk_set:\n",
        "            chunk_set.add(chunk.page_content)\n",
        "            deduped_chunks.append(chunk)\n",
        "    return deduped_chunks\n",
        "\n",
        "# The core function of the script, which orchestrates the creation of a FAISS database:\n",
        "# - Loading Documents: Utilizes DirectoryLoader to load documents from a specified directory. The documents are expected to be in JSON format.\n",
        "# - Splitting Documents: Documents are split into smaller chunks using a SentenceTransformersTokenTextSplitter, which leverages a specified embedding model.\n",
        "# - Deduplication: The chunks are deduplicated using the dedup_chunks function.\n",
        "# - Embedding Documents: Document chunks are embedded using the specified transformer model. Optionally, the model can be set to use half precision.\n",
        "# - Creating FAISS Index: A FAISS index is created from the deduplicated and embedded chunks, which is then saved locally.\n",
        "\n",
        "def create_db(\n",
        "    data_path: str = \"/data_fast/laws-lois-xml/documents\",\n",
        "    embedding_model: str = \"NousResearch/Meta-Llama-3-8B-Instruct-GGUF\",\n",
        "    save_path: str = \"/data_fast/laws-lois-xml/mxbai-embed-large-v1/faiss\",\n",
        "    half_precision: bool = False,\n",
        ") -> langchain_community.vectorstores.faiss.FAISS:\n",
        "    \"\"\"Create a faiss db from a directory of JSON files.\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to the directory of JSON files.\n",
        "        embedding_model: The HuggingFace model name to use for embeddings.\n",
        "        save_path: Path to save the db.\n",
        "        half_precision: Whether to use half precision for the embedding model.\n",
        "\n",
        "    Returns:\n",
        "        A faiss db.\n",
        "    \"\"\"\n",
        "    loader = langchain_community.document_loaders.DirectoryLoader(\n",
        "        data_path,\n",
        "        glob=\"**/*.json\",\n",
        "        loader_cls=langchain_community.document_loaders.JSONLoader,  # pyright: ignore[reportArgumentType]\n",
        "        loader_kwargs={\n",
        "            \"jq_schema\": \".\",\n",
        "            \"metadata_func\": metadata_func,\n",
        "            \"content_key\": \"article\",\n",
        "        },\n",
        "        use_multithreading=True,\n",
        "        recursive=True,\n",
        "    )\n",
        "    docs = loader.load()\n",
        "    with mock.patch.object(sentence_transformers, \"SentenceTransformer\", new=SentenceTransformer):\n",
        "        splitter = langchain.text_splitter.SentenceTransformersTokenTextSplitter(model_name=embedding_model)\n",
        "    chunks = splitter.split_documents(docs)\n",
        "    deduped_chunks = dedup_chunks(chunks)\n",
        "\n",
        "    embedder = langchain_community.embeddings.HuggingFaceEmbeddings(\n",
        "        model_name=embedding_model,\n",
        "        show_progress=True,\n",
        "        model_kwargs={\"trust_remote_code\": True},\n",
        "    )\n",
        "    assert isinstance(embedder.client, torch.nn.Module)\n",
        "    if half_precision:\n",
        "        embedder.client.half()\n",
        "    db = langchain_community.vectorstores.faiss.FAISS.from_documents(deduped_chunks, embedder)\n",
        "    db.save_local(save_path)\n",
        "    return db\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "2vq1c2YTiGHY",
        "outputId": "957f1012-59d9-44b2-d828-3385e2ad0456"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-49c36a30d571>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Hub and LangChain Installation"
      ],
      "metadata": {
        "id": "s20kEsqJqXyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For downloading the models from HF Hub\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS1Gae3ZqLtN",
        "outputId": "84d1cf68-3890-4bcc-fa12-bc45c6bd4c0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjXdIFpAqP-t",
        "outputId": "625aeadd-7a14-472a-ad31-c0bbd416430e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.19-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.38 (from langchain)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.52 (from langchain)\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.56-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.8/120.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.52->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.52->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.5 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.19 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.1 langsmith-0.1.56 marshmallow-3.21.2 mypy-extensions-1.0.0 orjson-3.10.3 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the Llama 3 8B GGUF model from NousResearch on Hugging Face\n",
        "\n",
        "The model we will be downloading from Hugging Face is a **5-bit quantized version of the Llama 3 8B chat model**, made available by NousResearch. The model is made available in the **GGUF format** - a new format introduced by the Llama CPP team and a replacement for the earlier GGML format, with advantages such as better tokenization and support for special tokens."
      ],
      "metadata": {
        "id": "e4UbzaDYmZSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download"
      ],
      "metadata": {
        "id": "_VLGnMN0mX9M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"NousResearch/Meta-Llama-3-8B-Instruct-GGUF\"\n",
        "model_basename = \"Meta-Llama-3-8B-Instruct-Q5_K_M.gguf\" # the model is in gguf format"
      ],
      "metadata": {
        "id": "lQKnttSLneNG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "ccc9e29f43b14a508a7303553f710a6d",
            "bfbb69333e474d59b7b0163a4b1363d7",
            "40cca674b6744d2f8336108ba1a74ba5",
            "0377e516454a4142be3b21b853d965c0",
            "d5f3587fb1e44df69e1d89e7a601678d",
            "c9c708bc5a8c4d6eaafaf990e984012d",
            "715abcd002244f22bec696b93346dbb2",
            "a734dc96f17949ceb7b79bac63e45ba9",
            "30c07e1c66a0426daec7e0e382c7379c",
            "2c0ba12f59b44e408b35654c23eeb77f",
            "6188adf2e4ba4d0291485f9a6582621b"
          ]
        },
        "id": "QDjsQ5FSqfMr",
        "outputId": "8778b660-0054-422c-8adc-2ded6ba23607"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Meta-Llama-3-8B-Instruct-Q5_K_M.gguf:   0%|          | 0.00/5.73G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccc9e29f43b14a508a7303553f710a6d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Llama 3 Model's Prompt Parameters\n",
        "\n",
        "We will utilize the Conversation Buffer Window Memory style from LangChain's Memory module. This style of memory only keeps a record of the last K chat interactions (in the style of a sliding window, in this example K = 5), and is hence a controllable and RAM-efficient way of storing memory for our LangChain agent."
      ],
      "metadata": {
        "id": "V_y7Dcr1p1sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# not really sure what to do here since this model isn't on the list of langchain integrations https://python.langchain.com/v0.1/docs/integrations/llms/"
      ],
      "metadata": {
        "id": "dVoGqlctp8C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B0zxR6hbf5d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Takeaways\n",
        "While the Llama 3 LangChain agent is definitely capable of providing answers and also using the external RAG vector store to compute the right answer to the prompt, the LangChain ReAct Prompt Template is very specific and it seems the LLM's chain does not stop even after it arrives at the correct answer.\n",
        "\n",
        "This is an issue with working with open-source LLMs in combination with LangChain on the free tier of Google Colab - due to the Colab GPU's 13 GB memory limit on the free tier, we are restricted to working with the 8B model of Llama 3, which is not as good at following instructions as OpenAI's GPT models."
      ],
      "metadata": {
        "id": "gxqhEsysoMkZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ccc9e29f43b14a508a7303553f710a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfbb69333e474d59b7b0163a4b1363d7",
              "IPY_MODEL_40cca674b6744d2f8336108ba1a74ba5",
              "IPY_MODEL_0377e516454a4142be3b21b853d965c0"
            ],
            "layout": "IPY_MODEL_d5f3587fb1e44df69e1d89e7a601678d"
          }
        },
        "bfbb69333e474d59b7b0163a4b1363d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9c708bc5a8c4d6eaafaf990e984012d",
            "placeholder": "​",
            "style": "IPY_MODEL_715abcd002244f22bec696b93346dbb2",
            "value": "Meta-Llama-3-8B-Instruct-Q5_K_M.gguf: 100%"
          }
        },
        "40cca674b6744d2f8336108ba1a74ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a734dc96f17949ceb7b79bac63e45ba9",
            "max": 5733500128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30c07e1c66a0426daec7e0e382c7379c",
            "value": 5733500128
          }
        },
        "0377e516454a4142be3b21b853d965c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c0ba12f59b44e408b35654c23eeb77f",
            "placeholder": "​",
            "style": "IPY_MODEL_6188adf2e4ba4d0291485f9a6582621b",
            "value": " 5.73G/5.73G [04:23&lt;00:00, 21.8MB/s]"
          }
        },
        "d5f3587fb1e44df69e1d89e7a601678d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9c708bc5a8c4d6eaafaf990e984012d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "715abcd002244f22bec696b93346dbb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a734dc96f17949ceb7b79bac63e45ba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c07e1c66a0426daec7e0e382c7379c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c0ba12f59b44e408b35654c23eeb77f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6188adf2e4ba4d0291485f9a6582621b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}