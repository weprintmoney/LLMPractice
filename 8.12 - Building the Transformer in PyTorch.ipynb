{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<center><p float=\"center\">\n","  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n","  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n","</p></center>\n","\n","<h1><center><font size=8>Transformer Deep Dive</center></font></h1>\n","<h1><center>Attention, Encoder, Decoder, Architecture</center></h1>\n","<h3><center>Charlcye Mitchell, April 2024</center></h3>"],"metadata":{"id":"WxKifhvwwyep"}},{"cell_type":"markdown","source":["# **Building the Transformer Model in PyTorch**"],"metadata":{"id":"poHr5nZ6guhv"}},{"cell_type":"markdown","source":["In this notebook, **we will utilize PyTorch** to implement the functions needed to construct the Decoder, and also put together the Encoder & Decoder stages needed to create the Transformer.\n","\n","**PyTorch is preferred for this implementation due to the increased flexibility it offers in creating custom functions and classes for advanced Deep Learning**, as opposed to TensorFlow. In the rest of this code notebook, we will attempt to explain the code in addition to the mathematical operations being performed for the Transformer."],"metadata":{"id":"7LIZL77kqE_B"}},{"cell_type":"markdown","source":["## **1. Importing the Libraries**"],"metadata":{"id":"JvUpcHebNCkk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KW8Kr15WC96"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np"]},{"cell_type":"markdown","source":["The **nn** subpackage in PyTorch is used to import the Neural Network module."],"metadata":{"id":"Xjmp9WScRftl"}},{"cell_type":"markdown","source":["## **2. The Self-Attention Block**"],"metadata":{"id":"IaM2tbVbRp_b"}},{"cell_type":"markdown","source":["- The Self-Attention Block is just the first class we are defining in this notebook.\n","- It implements the attention of one word with another word - this is identical to what we have done in the Encoder layer.\n","- Because this is done in PyTorch, we have greater freedom to define the functions in the manner required, here we have defined the Multi-head Attention function from scratch.\n","- The __init__() is a class declaration following OOP principles. In this declaration, we define an object blueprint with certain parameters (such as embed_size and heads in this example).\n","- In the rest of the initialization, we define the **W_k, W_q and W_v** matrices, which get multiplied with the embedding, to form our **K, Q and V vectors.**\n","- We scale these K, Q and V vectors to be of dimension: **embedding size * number of heads**, so that we can create the multiple K, Q & V vectors needed for Multi-head Attention (as opposed to just Self-Attention for one head)."],"metadata":{"id":"H8s12b-KHcHQ"}},{"cell_type":"code","source":["class SelfAttentionBlock(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(SelfAttentionBlock, self).__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","\n","\n","        self.W_v = nn.Linear(embed_size, self.heads*embed_size)\n","        self.W_k = nn.Linear(embed_size, self.heads*embed_size)\n","        self.W_q = nn.Linear(embed_size, self.heads*embed_size)\n","        self.fc = nn.Linear(self.heads*embed_size, embed_size)\n","\n","    def forward(self, embeddings, mask):\n","        # Get number of training examples\n","        N = embeddings.shape[0]\n","\n","        v_len, k_len, q_len = embeddings.shape[1], embeddings.shape[1], embeddings.shape[1]\n","\n","        V = self.W_v(embeddings)  # (N, value_len, heads*embed_size)\n","        K = self.W_k(embeddings)  # (N, key_len, heads*embed_size)\n","        Q = self.W_q(embeddings)  # (N, query_len, heads*embed_size)\n","\n","\n","        # Split the embedding into self.heads different pieces\n","        V = V.reshape(N, v_len, self.heads, self.embed_size)\n","        K = K.reshape(N, k_len, self.heads, self.embed_size)\n","        Q = Q.reshape(N, q_len, self.heads, self.embed_size)\n","\n","        qk = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])\n","        # queries shape: (N, query_len, heads, embed_size),\n","        # keys shape: (N, key_len, heads, embed_size)\n","        # energy: (N, heads, query_len, key_len)\n","\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","            qk = qk.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        attention = torch.softmax(qk / (self.embed_size ** (1 / 2)), dim=3)\n","        # attention shape: (N, heads, query_len, key_len)\n","\n","        output = torch.einsum(\"nhql,nlhd->nqhd\", [attention, V]).reshape(\n","            N, q_len, self.heads * self.embed_size\n","        )\n","        # attention shape: (N, heads, query_len, key_len)\n","        # values shape: (N, value_len, heads, embed_size)\n","        # out after matrix multiply: (N, query_len, heads, embed_size), then\n","        # we reshape and flatten the last two dimensions.\n","\n","        out = self.fc(output)\n","\n","        return out\n"],"metadata":{"id":"bM6B2x_jYBVB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **Now in the forward() function being defined**, we are performing the computations for the Attention mechanism. We are multiplying the Query of one word with the Keys of every other word in the sentence.\n","- Then we scale it by dividing by a factor, and perform softmax along the dimension which contains the QK values (the Attention scores). These softmax scores add to 1 of course, and they are each multiplied by the Value vectors of each word in the sentence.\n","- It's important to note from the code that **this is being accomplished in a matrix multiplication manner**, where the dimensions of the queries, keys and values are defined such that we directly get the final Multi-head Attention matrix. **This is what the einsum() function is helping us do in the code.**\n","\n","- In the SelfAttention() class, we had defined another fully connected or **fc layer**, which acts now to convert the shape of the Multi-head Attention matrix, to the shape of a single Self-Attention head."],"metadata":{"id":"vU9W5xnDXapG"}},{"cell_type":"markdown","source":["**Note:** There is an additional PyTorch technicality to be aware of. When we inherit from the nn.Module, there is a function written in the \"Module\" library which forces the forward function to be called when we call the class name. Therefore, with the help of super(SelfAttention, self).__init__(), we are inheriting directly from the nn.Module() library.\n","\n","**Note 2:** There is a mask padding defined in the forward() function, which is only relevant when we use the Self-Attention in the Decoder block. We will see later how that works."],"metadata":{"id":"w-MRgz1rXypf"}},{"cell_type":"markdown","source":["## **3. The Encoder Block**"],"metadata":{"id":"X4sgx15KX6OF"}},{"cell_type":"code","source":["class EncoderBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout):\n","        super(EncoderBlock, self).__init__()\n","        self.attention_mechanism = SelfAttentionBlock(embed_size, heads)\n","        self.normalization1 = nn.LayerNorm(embed_size)\n","        self.normalization2 = nn.LayerNorm(embed_size)\n","\n","        self.feed_forward_layers = nn.Sequential(\n","            nn.Linear(embed_size, 4 * embed_size),\n","            nn.ReLU(),\n","            nn.Linear(4 * embed_size, embed_size),\n","        )\n","\n","        self.dropout_layer = nn.Dropout(dropout)\n","\n","    def forward(self, embeddings, mask):\n","        attention_output = self.attention_mechanism(embeddings, mask)\n","\n","        # Add skip connection, run through normalization and finally dropout\n","        x = self.dropout_layer(self.normalization1(attention_output + embeddings))\n","        forward = self.feed_forward_layers(x)\n","        output = self.dropout_layer(self.normalization2(forward + x))\n","        return output\n"],"metadata":{"id":"WjbOK2wai7Ut"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- The **EncoderBlock** class contains the layers with which we can complete the rest of the Encoder block.\n","- We are first importing the Self-Attention mechanism we had built earlier, then we are adding a few LayerNorm, Dropout and Linear layers.\n","- The order of these layers is defined in the forward() function - we are first implementing Self-Attention on the input embeddings (which will already contain the sum of the positional encodings and word embeddings) to the TransformerBlock.\n","- This is followed by adding the Attention scores to the initial embeddings in a Skip & Add manner, followed by Layer Normalization, and finally a Dropout mechanism. This sequence can be described by the Encoder block picture we have already seen.\n","- Finally, there's a Feed-Forward Neural Network of 2 fully-connected layers (with the first layer having a ReLU activation function).\n","- The first linear layer scales up the dimension to 4 times the embedding size (the number 4 is just a hyperparameter), and the second layer shrinks the dimensions back to the embedding size.\n","- There is a lot of research around why Neural Networks are shaped in this manner (expansion + contraction), but suffice it to say, **this shape helps Neural Nets learn high-quality representations of their inputs.**"],"metadata":{"id":"3fq6TzIWOV96"}},{"cell_type":"markdown","source":["## **4. The Encoder Stack**"],"metadata":{"id":"xctjEa-PcIdV"}},{"cell_type":"code","source":["class EncoderStack(nn.Module):\n","    def __init__(\n","        self,\n","        source_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        device,\n","        dropout,\n","        max_length,\n","    ):\n","\n","        super(EncoderStack, self).__init__()\n","        self.embed_size = embed_size\n","        self.device = device\n","        self.words_embedding = nn.Embedding(source_vocab_size, embed_size)\n","        self.positional_embedding = nn.Embedding(max_length, embed_size)\n","\n","        self.EncoderBlocklayers = nn.ModuleList(\n","            [\n","                EncoderBlock(\n","                    embed_size,\n","                    heads,\n","                    dropout=dropout\n","                )\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        N, seq_length = x.shape\n","        positions_of_words = torch.arange(0, seq_length)\n","        positional_encoding = []\n","        for i in range(len(positions_of_words)):\n","          positional_encoding.append(np.cos(2*np.pi*.73*i))\n","        positional_encoding = torch.Tensor(positional_encoding)\n","        positional_encoding = positional_encoding.expand(N, seq_length)\n","        positional_encoding = positional_encoding.to(self.device)\n","        positional_encoding = positional_encoding.type(torch.int64)\n","        output = self.dropout(\n","            (self.words_embedding(x) + self.positional_embedding(positional_encoding))\n","        )\n","\n","        # In the Encoder the query, key, value are all the same, it's in the\n","        # decoder this will change. This might look a bit odd in this case.\n","\n","\n","        for each_layer in self.EncoderBlocklayers:\n","            output = each_layer(output, mask)\n","\n","        return output"],"metadata":{"id":"kYz3H-XNlYOA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- In this code block, we are creating the **Encoder Stack** of the Transformer, **containing as many Encoder blocks as defined** by the num_layers parameter in the __init__() declaration.\n","- We are also defining the Word Embedding and Positional Embedding layers in this step.\n","- **The nn.Embedding() function is being trained (it has learnable parameters) to produce unique vectors for each unique index that it receives.** We use this function to create two kinds of embedding layers - the Word Embeddings & Position Embeddings.\n","- These two embeddings are added and then fed to the layers that we built in the EncoderBlock() class."],"metadata":{"id":"g5EDWkqDR59x"}},{"cell_type":"markdown","source":["## **5. The Encoder-Decoder Attention Layer**"],"metadata":{"id":"OzyY1yGYcghy"}},{"cell_type":"code","source":["class Encoder_Decoder_Attention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super(Encoder_Decoder_Attention, self).__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","\n","\n","        self.W_Encoder_Decoder_values = nn.Linear(embed_size, self.heads*embed_size)\n","        self.W_Encoder_Decoder_keys = nn.Linear(embed_size, self.heads*embed_size)\n","        self.W_self_queries = nn.Linear(embed_size, self.heads*embed_size)\n","        self.fc = nn.Linear(self.heads*embed_size, embed_size)\n","\n","    def forward(self, embeddings,encoder_outputs):\n","        # Get number of training examples\n","        N = embeddings.shape[0]\n","\n","        v_len, k_len, q_len = encoder_outputs.shape[1], encoder_outputs.shape[1], embeddings.shape[1]\n","\n","        V = self.W_Encoder_Decoder_values(encoder_outputs)  # (N, value_len, heads*embed_size)\n","        K = self.W_Encoder_Decoder_keys(encoder_outputs)  # (N, key_len, heads*embed_size)\n","        Q = self.W_self_queries(embeddings)  # (N, query_len, heads*embed_size)\n","\n","\n","        # Split the embedding into self.heads different pieces\n","        V = V.reshape(N, v_len, self.heads, self.embed_size)\n","        K = K.reshape(N, k_len, self.heads, self.embed_size)\n","        Q = Q.reshape(N, q_len, self.heads, self.embed_size)\n","\n","        qk = torch.einsum(\"nqhd,nkhd->nhqk\", [Q, K])\n","        # queries shape: (N, query_len, heads, embed_size),\n","        # keys shape: (N, key_len, heads, embed_size)\n","        # energy: (N, heads, query_len, key_len)\n","\n","        attention = torch.softmax(qk / (self.embed_size ** (1 / 2)), dim=3)\n","        # attention shape: (N, heads, query_len, key_len)\n","\n","        output = torch.einsum(\"nhql,nlhd->nqhd\", [attention, V]).reshape(\n","            N, q_len, self.heads * self.embed_size\n","        )\n","        # attention shape: (N, heads, query_len, key_len)\n","        # values shape: (N, value_len, heads, embed_size)\n","        # out after matrix multiply: (N, query_len, heads, embed_size), then\n","        # we reshape and flatten the last two dimensions.\n","\n","        encoder_outputs = self.fc(output)\n","\n","        return encoder_outputs"],"metadata":{"id":"c_2szgnowtpD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- **The Encoder-Decoder Attention layer is the unique layer** that Decoders possess outside what's already in the Encoder layer.\n","- We have declared a Wq matrix that converts the embeddings on the Decoder side into queries. However, **the Wk and Wv matrices we declare here are different from before, as they are working on the Encoder's outputs and not on the embeddings received from the previous layer in the Decoder side of things.**\n","- **The Query has been computed from the previous embeddings inputed into the layer, whereas the Keys and Values are computed from the final Encoder outputs themselves.**\n","- The rest of it is the same as the Self-Attention mechanism."],"metadata":{"id":"jVKfe51CUtTo"}},{"cell_type":"markdown","source":["## **6. The Decoder Block**"],"metadata":{"id":"tWbr6iblc5bk"}},{"cell_type":"code","source":["class DecoderBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, device):\n","        super(DecoderBlock, self).__init__()\n","        self.normalization1 = nn.LayerNorm(embed_size)\n","        self.normalization2 = nn.LayerNorm(embed_size)\n","        self.attention_layer = SelfAttentionBlock(embed_size, heads=heads)\n","        self.EncoderDecoderAttention = Encoder_Decoder_Attention(embed_size, heads=heads)\n","        self.transformer_block = EncoderBlock(\n","            embed_size, heads, dropout\n","        )\n","        self.dropoutlayer1 = nn.Dropout(dropout)\n","        self.dropoutlayer2 = nn.Dropout(dropout)\n","\n","\n","\n","\n","    def forward(self, x, encoder_outputs, src_mask, trg_mask):\n","        attention_output = self.attention_layer(x, trg_mask)\n","        out = self.dropoutlayer1(self.normalization1(attention_output + x))\n","        EncoderDecoderAttentionoutput = self.EncoderDecoderAttention(out, encoder_outputs)\n","        out2 = self.dropoutlayer2(self.normalization2(EncoderDecoderAttentionoutput + out))\n","        return out2"],"metadata":{"id":"zcES7p9fp5BQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- The Decoder Block is the counterpart to the Encoder Block we defined before.\n","- Since the Encoder and Decoder blocks show a lot of similarity, we are using some layers from the EncoderBlock Class.\n","- However the difference is of course that inside Decoders, they implement both Self-Attention and Encoder-Decoder Attention.\n","- We set the trg_mask (Target Mask) inside the Self-Attention block, such that it does not look at the time steps ahead during Training. We do that by setting these word embeddings to zero."],"metadata":{"id":"FKLKZS8WaleT"}},{"cell_type":"markdown","source":["## **7. The Decoder Stack**"],"metadata":{"id":"AP0AVSOfdRP0"}},{"cell_type":"code","source":["class DecoderStack(nn.Module):\n","    def __init__(\n","        self,\n","        target_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        dropout,\n","        device,\n","        max_length,\n","    ):\n","        super(DecoderStack, self).__init__()\n","        self.device = device\n","        self.words_embedding = nn.Embedding(target_vocab_size, embed_size)\n","        self.positional_embedding = nn.Embedding(max_length, embed_size)\n","\n","        self.DecoderBlocklayers = nn.ModuleList(\n","            [\n","                DecoderBlock(embed_size, heads, dropout, device)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","        self.fc_out = nn.Linear(embed_size, target_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_out, src_mask, trg_mask):\n","        N, seq_length = x.shape\n","        positions_of_words = torch.arange(0, seq_length)\n","        positional_encoding = []\n","        for i in range(len(positions_of_words)):\n","          positional_encoding.append(np.cos(2*np.pi*.73*i))\n","        positional_encoding = torch.Tensor(positional_encoding)\n","        positional_encoding = positional_encoding.expand(N, seq_length)\n","        positional_encoding = positional_encoding.to(self.device)\n","        positional_encoding = positional_encoding.type(torch.int64)\n","        x = self.dropout((self.words_embedding(x) + self.positional_embedding(positional_encoding)))\n","\n","        for each_layer in self.DecoderBlocklayers:\n","            x = each_layer(x, enc_out, src_mask, trg_mask)\n","\n","        out = self.fc_out(x)\n","\n","        return out\n","\n","\n",""],"metadata":{"id":"TvUM8SiH1w31"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- This DecoderStack() is doing what the EncoderStack() was doing, just on the Decoder side of things.\n","- **There is an additional fully-connected group of layers afterwards**, which perform the classification task to classify between the tokens / words in the vocabulary."],"metadata":{"id":"l4VgaNOvbzjG"}},{"cell_type":"markdown","source":["## **8. The Transformer Architecture**"],"metadata":{"id":"dAkdNcGMdkdf"}},{"cell_type":"code","source":["class TransformerArchitecture(nn.Module):\n","    def __init__(\n","        self,\n","        source_vocab_size,\n","        target_vocab_size,\n","        source_pad_idx,\n","        target_pad_idx,\n","        embed_size=512,\n","        num_layers=6,\n","        heads=8,\n","        dropout=0,\n","        device=\"cpu\",\n","        max_length=100,\n","    ):\n","\n","        super(TransformerArchitecture, self).__init__()\n","\n","        self.encoder = EncoderStack(\n","            source_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            device,\n","            dropout,\n","            max_length,\n","        )\n","\n","        self.decoder = DecoderStack(\n","            target_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            dropout,\n","            device,\n","            max_length,\n","        )\n","\n","        self.source_pad_idx = source_pad_idx\n","        self.target_pad_idx = target_pad_idx\n","        self.device = device\n","\n","    def make_source_mask(self, src):\n","        src_mask_pads = (src != self.source_pad_idx).unsqueeze(1).unsqueeze(2)\n","        # (N, 1, 1, src_len)\n","        return src_mask_pads.to(self.device)\n","\n","    def make_target_mask(self, trg):\n","        N, trg_len = trg.shape\n","        trg_mask_pads = torch.tril(torch.ones((trg_len, trg_len))).expand(\n","            N, 1, trg_len, trg_len\n","        )\n","\n","        return trg_mask_pads.to(self.device)\n","\n","    def forward(self, src, trg):\n","        source_mask_pad = self.make_source_mask(src)\n","        target_mask_pad = self.make_target_mask(trg)\n","        enc_src = self.encoder(src, None)\n","        out = self.decoder(trg, enc_src, source_mask_pad, target_mask_pad)\n","        return out"],"metadata":{"id":"hRRuSNjI5G5e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- This class puts all the pieces together from the Encoder stack and the Decoder stack.\n","- We also implement Target masks, which dynamically mask words ahead of the current time-step, while feeding it to the Self-Attention block in the Decoder."],"metadata":{"id":"hMVqvdDodYAO"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","x = torch.tensor([[2, 4, 5, 1, 3, 7, 2, 1, 3], [1, 3, 6, 7, 2, 9, 2, 5, 8]]).to(\n","        device)\n","trg = torch.tensor([[0, 3, 5, 4, 1, 3, 2, 5, 8], [2, 3, 1, 0, 5, 9, 4, 9, 7]]).to(device)\n","\n","source_pad_idx = 0\n","target_pad_idx = 0\n","source_vocab_size = 10\n","target_vocab_size = 10\n","model = TransformerArchitecture(source_vocab_size, target_vocab_size, source_pad_idx, target_pad_idx, device=device).to(\n","        device\n","    )\n","out = model(x, trg[:, :-1])\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63D46V6Sbnef","outputId":"9baf6329-c0d4-486c-f0f5-9421bb451987"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n","torch.Size([2, 8, 10])\n"]}]},{"cell_type":"markdown","source":["## **9. Summary & Conclusions**"],"metadata":{"id":"YS7M2bX1d5Ke"}},{"cell_type":"markdown","source":["- This was a **quick code demonstration** of how the Transformer architecture can be put together from scratch by utilizing the concepts we have discussed about both the Encoder and the Decoder.\n","- While this notebook was meant to give a code-based understanding of the building blocks of the Transformer model, in actual practice, **the industry operates at the level of large-scale architectures themselves.**\n","- That means, once the fundamentals of the architecture are mastered, it will be more relevant moving forward to understand the large models that use these blocks in multiple ways, and know how to import pre-trained models and fine-tune them where required. This will be the focus of our deep-dive into modern Transformer architectures moving forward."],"metadata":{"id":"rfahvV2Nd0Uh"}}]}